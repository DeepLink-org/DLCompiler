diff --git a/include/triton-shared/Conversion/TritonArithToLinalg/ConversionPatterns.hpp b/include/triton-shared/Conversion/TritonArithToLinalg/ConversionPatterns.hpp
index 1d9244d..6fc0df1 100644
--- a/include/triton-shared/Conversion/TritonArithToLinalg/ConversionPatterns.hpp
+++ b/include/triton-shared/Conversion/TritonArithToLinalg/ConversionPatterns.hpp
@@ -675,45 +675,116 @@ struct BroadcastConverter : public OpConversionPattern<triton::BroadcastOp> {
 private:
   using OpConversionPattern<triton::BroadcastOp>::OpConversionPattern;
 
+
+  SmallVector<int64_t> getBroadcastDims(RankedTensorType src,
+                                        RankedTensorType dst) const {
+    SmallVector<int64_t> broadcastDims;
+    auto srcShape = src.getShape();
+    auto dstShape = dst.getShape();
+
+    for (size_t i = 0; i < srcShape.size(); i++) {
+      if (dstShape[i] != srcShape[i]) {
+        assert(srcShape[i] == 1);
+        broadcastDims.push_back(i);
+      }
+    }
+    assert(!broadcastDims.empty() && "cannot identify broadcast dimension");
+    return broadcastDims;
+  }
+
+  // Broadcasts input tensor based on TosaToLinalg's broadcastToShape
+  AffineMap getBroadcastAffineMap(MLIRContext *context,
+                                  ArrayRef<int64_t> inputShape,
+                                  ArrayRef<int64_t> broadcastToShape) const {
+
+    assert(broadcastToShape.size() >= inputShape.size());
+
+    // Create affine map and shapes for tensor initialization.
+    SmallVector<AffineExpr> outExpr;
+
+    size_t diff = broadcastToShape.size() - inputShape.size();
+    for (size_t i = 0; i < broadcastToShape.size(); i++) {
+      if (i < diff) {
+        continue;
+      }
+      size_t j = i - diff;
+      if (inputShape[j] == 1) {
+        // Broadcast singleton dimension
+        outExpr.push_back(mlir::getAffineConstantExpr(0, context));
+        continue;
+      }
+      // Non-broadcast case
+      outExpr.push_back(mlir::getAffineDimExpr(i, context));
+    }
+    return AffineMap::get(broadcastToShape.size(), 0, outExpr, context);
+  }
+
 public:
+  // Dimensions of collapesd tensor is all unbroadcast dims
+  SmallVector<int64_t> getUnbroadcastDims(RankedTensorType src,
+                                          RankedTensorType dst) const {
+    SmallVector<int64_t> unbroadcastDims;
+    auto srcShape = src.getShape();
+    auto dstShape = dst.getShape();
+
+    for (size_t i = 0; i < srcShape.size(); ++i) {
+      if (dstShape[i] == srcShape[i]) {
+        unbroadcastDims.emplace_back(srcShape[i]);
+      }
+    }
+    return unbroadcastDims;
+  }
+  // Here convert tt.broadcast to linalg.broadcast
+  //
+  // before
+  // %out = tt.broadcast %in : tensor<1x4x8xf32> -> tensor<128x4x8xf32>
+  //
+  // after
+  // %collpased = tensor.collapse_shape %in [[0, 1], [2]] :
+  //                                    tensor<1x4x8xf32> into tensor<4x8xf32>
+  // %out = linalg.broadcast ins(%collpased : tensor<4x8xf32>)
+  //                         outs(%empty : tensor<128x4x8xf32>) dimensions = [0]
   LogicalResult
   matchAndRewrite(triton::BroadcastOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
-    auto loc = op.getLoc();
+    assert(op->getNumResults() == 1 && "BroadcastOp assumes single result");
 
-    assert(op->getNumResults() == 1 && "code assumes single result!");
     RankedTensorType sourceType =
         cast<RankedTensorType>(adaptor.getSrc().getType());
     RankedTensorType resultType = cast<RankedTensorType>(op.getType());
     auto elementType = resultType.getElementType();
     size_t resultRank = resultType.getRank();
+    auto loc = op.getLoc();
+
+    auto initEmpty =
+        rewriter.create<tensor::EmptyOp>(loc, resultType.getShape(), elementType);
 
-    SmallVector<AffineMap> indexingMaps;
-    indexingMaps.reserve(op->getNumOperands() + op->getNumResults());
+    SmallVector<int64_t> broadcastDims =
+        getBroadcastDims(sourceType, resultType);
+    SmallVector<int64_t> unbroadcastDims =
+        getUnbroadcastDims(sourceType, resultType);
 
-    indexingMaps.push_back(getBroadcastAffineMap(
-        op->getContext(), sourceType.getShape(), resultType.getShape()));
-    indexingMaps.append(op->getNumResults(),
-                        rewriter.getMultiDimIdentityMap(resultRank));
+    SmallVector<ReassociationIndices> collapseReassociationIndices;
+    auto collapseReassociationIndicesOptional =
+        getReassociationIndicesForCollapse(sourceType.getShape(),
+                                          unbroadcastDims);
+    if (!collapseReassociationIndicesOptional.has_value()) {
+      return rewriter.notifyMatchFailure(
+          op, "Failure with getReassociationIndicesForCollapse call");
+    }
+    collapseReassociationIndices = collapseReassociationIndicesOptional.value();
 
-    assert(op->getNumResults() == 1 && "code assumes single result!");
-    auto init = rewriter.create<tensor::EmptyOp>(loc, resultType.getShape(),
-                                                 elementType);
+    RankedTensorType collapseResultType =
+        RankedTensorType::get(unbroadcastDims, sourceType.getElementType());
 
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc, op->getResultTypes(), ValueRange{adaptor.getSrc()},
-        ValueRange{init}, indexingMaps, getNParallelLoopsAttrs(resultRank),
-        [&](OpBuilder &nestedBuilder, Location nestedLoc,
-            ValueRange blockArgs) {
-          Value opResult = blockArgs[0];
-          nestedBuilder.create<linalg::YieldOp>(loc, opResult);
-        });
+    auto collpasedOp = rewriter.create<tensor::CollapseShapeOp>(
+        loc, collapseResultType, adaptor.getSrc(), collapseReassociationIndices);
 
-    linalgOp->setAttr("broadcastDims",
-                      rewriter.getDenseI64ArrayAttr(
-                          getBroadcastDims(sourceType, resultType)));
+    auto broadcastOp = rewriter.create<linalg::BroadcastOp>(
+        loc, collpasedOp, initEmpty,
+        rewriter.getDenseI64ArrayAttr(broadcastDims));
 
-    rewriter.replaceOp(op, linalgOp->getResults());
+    rewriter.replaceOp(op, broadcastOp.getResults());
     return success();
   }
 };
diff --git a/lib/Conversion/TritonArithToLinalg/TritonArithToLinalg.cpp b/lib/Conversion/TritonArithToLinalg/TritonArithToLinalg.cpp
index 44ecd72..3c4e80d 100644
--- a/lib/Conversion/TritonArithToLinalg/TritonArithToLinalg.cpp
+++ b/lib/Conversion/TritonArithToLinalg/TritonArithToLinalg.cpp
@@ -101,5 +101,6 @@ void mlir::triton::populateTritonArithToLinalgConversionPatterns(
 
   // Note: the ordering here matters!
   // These patterns are added last to they will be tried last.
-  linalg::populateElementwiseToLinalgConversionPatterns(patterns);
+  // linalg::populateElementwiseToLinalgConversionPatterns(patterns);
+  llvm::errs() << "zmz debug experiment: skip elementwise to linalg conversion\n";
 }
diff --git a/lib/Conversion/TritonArithToLinalg/TritonArithToLinalgPass.cpp b/lib/Conversion/TritonArithToLinalg/TritonArithToLinalgPass.cpp
index 8c86fe2..8f16a44 100644
--- a/lib/Conversion/TritonArithToLinalg/TritonArithToLinalgPass.cpp
+++ b/lib/Conversion/TritonArithToLinalg/TritonArithToLinalgPass.cpp
@@ -146,7 +146,8 @@ public:
                 return isa<RankedTensorType>(type);
               });
 
-          return !operateOnTensors;
+          // return !operateOnTensors;
+          return true;
         });
 
     if (pidsToFuncArgs) {
@@ -194,14 +195,17 @@ public:
         addProgramInfo(func);
       }
     }
+    llvm::errs() << "zmz debug After adding program info\n";
 
     if (failed(applyPartialConversion(moduleOp, target, std::move(patterns)))) {
       signalPassFailure();
     }
+    llvm::errs() << "zmz debug After applyPartialConversion\n";
 
     if (failed(applyTensorConcatDecomposition())) {
       signalPassFailure();
     }
+    llvm::errs() << "zmz debug After applyTensorConcatDecomposition\n";
 
     // Convert tt.func and tt.return into func's counterparts
     if (ttToFuncFunc) {
@@ -241,6 +245,7 @@ public:
         func.erase();
       });
     }
+    llvm::errs() << "zmz debug After ttToFuncFunc\n";
   }
 };
 
diff --git a/lib/Conversion/TritonToLinalg/TritonToLinalg.cpp b/lib/Conversion/TritonToLinalg/TritonToLinalg.cpp
index 1c8ed9c..9fa042e 100644
--- a/lib/Conversion/TritonToLinalg/TritonToLinalg.cpp
+++ b/lib/Conversion/TritonToLinalg/TritonToLinalg.cpp
@@ -91,5 +91,6 @@ void mlir::triton::populateTritonToLinalgConversionPatterns(
   // will be tried last. Incorrect ordering or having MetaOpConverter has lower
   // PatternBenefit will result in element-wise meta ops being converted to
   // linalg.generic ops.
-  linalg::populateElementwiseToLinalgConversionPatterns(patterns);
+//   linalg::populateElementwiseToLinalgConversionPatterns(patterns);
+    llvm::errs() << "zmz debug skip populateElementwiseToLinalgConversionPatterns\n";
 }
diff --git a/lib/Conversion/TritonToLinalg/TritonToLinalgPass.cpp b/lib/Conversion/TritonToLinalg/TritonToLinalgPass.cpp
index 25b7db8..d31b410 100644
--- a/lib/Conversion/TritonToLinalg/TritonToLinalgPass.cpp
+++ b/lib/Conversion/TritonToLinalg/TritonToLinalgPass.cpp
@@ -172,7 +172,7 @@ public:
                 return isa<RankedTensorType>(type);
               });
 
-          return !operateOnTensors;
+          return true;
         });
 
     triton::populateTritonToLinalgConversionPatterns(
